{"cells":[{"cell_type":"markdown","metadata":{"id":"_H5RHwtBC1Hf"},"source":["Following: https://predictivehacks.com/?all-tips=how-to-get-bert-embeddings-with-tensorflow-hub"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"executionInfo":{"elapsed":8367,"status":"ok","timestamp":1700625240456,"user":{"displayName":"Annie Lyu","userId":"12460954671218305598"},"user_tz":-780},"id":"C2q51CTjC06H","outputId":"1ddde6c4-e919-4be3-8091-2f6d33862003"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting numpy==1.23\n","  Downloading numpy-1.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.23.5\n","    Uninstalling numpy-1.23.5:\n","      Successfully uninstalled numpy-1.23.5\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\n","tensorflow 2.14.0 requires numpy>=1.23.5, but you have numpy 1.23.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.23.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]}}},"metadata":{}}],"source":["!pip install numpy==1.23"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":384,"status":"ok","timestamp":1684213124089,"user":{"displayName":"Annie Lyu","userId":"12460954671218305598"},"user_tz":-720},"id":"8Zf2HjJw67tl","outputId":"10a31ea3-be4f-49f5-9422-800dc159bd96"},"outputs":[{"name":"stdout","output_type":"stream","text":["ANSI_X3.4-1968\n"]}],"source":["import locale\n","print(locale.getpreferredencoding())\n","\n","def getpreferredencoding(do_setlocale = True):\n","    return \"UTF-8\"\n","locale.getpreferredencoding = getpreferredencoding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LJYpssTV69o6"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ShX6o_o_DWxY","outputId":"c021b272-c876-4cda-cd15-fbdff2b898fe","executionInfo":{"status":"ok","timestamp":1700625296998,"user_tz":-780,"elapsed":56547,"user":{"displayName":"Annie Lyu","userId":"12460954671218305598"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-datasets 4.9.3 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n","tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement opencv-python-headless==4.5.2.52 (from tf-models-official) (from versions: 3.4.10.37, 3.4.11.39, 3.4.11.41, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.15.55, 3.4.16.59, 3.4.17.61, 3.4.17.63, 3.4.18.65, 4.3.0.38, 4.4.0.40, 4.4.0.42, 4.4.0.44, 4.4.0.46, 4.5.1.48, 4.5.3.56, 4.5.4.58, 4.5.4.60, 4.5.5.62, 4.5.5.64, 4.6.0.66, 4.7.0.68, 4.7.0.72, 4.8.0.74, 4.8.0.76, 4.8.1.78)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for opencv-python-headless==4.5.2.52\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install -q -U \"tensorflow-text==2.11.*\"\n","!pip install -q tf-models-official==2.11.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9264,"status":"ok","timestamp":1700625306256,"user":{"displayName":"Annie Lyu","userId":"12460954671218305598"},"user_tz":-780},"id":"w1KI2plF73J_","outputId":"81daf886-a537-4659-e84b-3c5410873f53"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting geomloss[full]\n","  Downloading geomloss-0.2.6.tar.gz (26 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from geomloss[full]) (1.23.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from geomloss[full]) (2.1.0+cu118)\n","Collecting pykeops (from geomloss[full])\n","  Downloading pykeops-2.1.2.tar.gz (88 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.9/88.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pybind11 (from pykeops->geomloss[full])\n","  Downloading pybind11-2.11.1-py3-none-any.whl (227 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.7/227.7 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting keopscore==2.1.2 (from pykeops->geomloss[full])\n","  Downloading keopscore-2.1.2.tar.gz (84 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->geomloss[full]) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->geomloss[full]) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->geomloss[full]) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->geomloss[full]) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->geomloss[full]) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->geomloss[full]) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->geomloss[full]) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->geomloss[full]) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->geomloss[full]) (1.3.0)\n","Building wheels for collected packages: geomloss, pykeops, keopscore\n","  Building wheel for geomloss (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for geomloss: filename=geomloss-0.2.6-py3-none-any.whl size=32245 sha256=a55334cc35a478ebe179e9069dbe454abb8cc9ceab43a88b1588be9c0f810cb9\n","  Stored in directory: /root/.cache/pip/wheels/0d/c9/80/4387eb03aa215ae557869d6fe8be498fd3d3cf297db2357b67\n","  Building wheel for pykeops (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pykeops: filename=pykeops-2.1.2-py3-none-any.whl size=114071 sha256=039a2f33efa9be7426e3d4bc7adb5a13dd7eacdde09e7d582ae7b1000e9df73f\n","  Stored in directory: /root/.cache/pip/wheels/93/91/9e/279e56403818cf05d868c2d90a13bde97572bcd11673d6e8ef\n","  Building wheel for keopscore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keopscore: filename=keopscore-2.1.2-py3-none-any.whl size=146447 sha256=300f3248bd56e26a7166035eb48d29e87d8ac2d0cc566c2d6cf20078b98a84d8\n","  Stored in directory: /root/.cache/pip/wheels/63/ac/b7/75fb4d24be97d9a930905eddf822cb765ca204b83df7aeaaa9\n","Successfully built geomloss pykeops keopscore\n","Installing collected packages: pybind11, keopscore, pykeops, geomloss\n","Successfully installed geomloss-0.2.6 keopscore-2.1.2 pybind11-2.11.1 pykeops-2.1.2\n"]}],"source":["!pip install geomloss[full]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16983,"status":"ok","timestamp":1690344981499,"user":{"displayName":"Annie Lyu","userId":"12460954671218305598"},"user_tz":-720},"id":"mRsOquaj6Eqq","outputId":"74cd8b18-f56a-41ce-fb3d-994d2d2eadd9"},"outputs":[{"output_type":"stream","name":"stdout","text":["[KeOps] Compiling cuda jit compiler engine ... OK\n","[pyKeOps] Compiling nvrtc binder for python ... OK\n","[KeOps] Generating code for formula Sum_Reduction((Var(0,3,0)-Var(1,3,1))|(Var(0,3,0)-Var(1,3,1)),1) ... OK\n","pyKeOps with numpy bindings is working!\n"]}],"source":["import pykeops\n","pykeops.test_numpy_bindings()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L7tYUk-Gx2kj"},"outputs":[],"source":["!pip install pykeops > install.log"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KW0sPT95DXFX"},"outputs":[],"source":["import os\n","import shutil\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","#from official.nlp import optimization  # to create AdamW optimizer\n","\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H0KM6XFEw1_C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700625326838,"user_tz":-780,"elapsed":16791,"user":{"displayName":"Annie Lyu","userId":"12460954671218305598"}},"outputId":"88219652-6e54-4f23-84d3-bc60729f225f"},"outputs":[{"output_type":"stream","name":"stdout","text":["[KeOps] Compiling cuda jit compiler engine ... OK\n","[pyKeOps] Compiling nvrtc binder for python ... OK\n"]}],"source":["import gensim\n","import tensorflow as tf\n","#import tensorflow.compat.v1 as tf\n","import numpy as np\n","#from gensim.models import KeyedVectors\n","#os.chdir(\"/home/jlu851/recap\")\n","import torch\n","import geomloss\n","from geomloss import SamplesLoss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n8lUfQXgx02V"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WFmZW77SA2rt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700627097610,"user_tz":-780,"elapsed":24868,"user":{"displayName":"Annie Lyu","userId":"12460954671218305598"}},"outputId":"11aac273-8736-409e-e8b4-b1ed7a3b2c7f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","source":["%cd \"/content/gdrive/My Drive/reuters/hedwig-master/hedwig-data/datasets/Reuters\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sjv4DrKYXzp2","executionInfo":{"status":"ok","timestamp":1700366398844,"user_tz":-780,"elapsed":2557,"user":{"displayName":"Annie Lyu","userId":"12460954671218305598"}},"outputId":"51b9b7d7-85b9-4fd5-d451-5f994234d149"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/reuters/hedwig-master/hedwig-data/datasets/Reuters\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","summary_df = pd.read_csv(\"/content/gdrive/My Drive/new results/bert2-za.csv\")"],"metadata":{"id":"q13HLfsXvzY2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1X1ko5ZkGWEI"},"source":["Loading models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E3fzoQ9xE5VR"},"outputs":[],"source":["#pretrained Bert model\n","bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess, trainable=True)\n","\n","#fine-tuned models:\n","#glue_model = tf.saved_model.load('/content/gdrive/My Drive/sst2/bert-ft-glue-tensorflow-epoch5')"]},{"cell_type":"markdown","metadata":{"id":"rO6ffyTOJW9p"},"source":["Get embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tQLkzIKdxYHZ"},"outputs":[],"source":["#def sampling (tweets_list, model, size = 810): #test with general bert model first\n","#def sampling (tweets_list, model, size = 810, stratify = False):\n","#  rands_text, rands_lab = resample(samp_50k, samp_50k_lab, random_state = 327, n_samples = 810)\n","#  strat_text, strat_lab = resample(samp_50k, samp_50k_lab, random_state = 327, n_samples = 810, stratify = df_50k[\"country\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EBn40rG-UNP6"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MpNOuzQoyxsR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700627103290,"user_tz":-780,"elapsed":1615,"user":{"displayName":"Annie Lyu","userId":"12460954671218305598"}},"outputId":"48762124-6489-4a6b-833f-04cda3223a78"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/reuters/hedwig-master/hedwig-data/datasets/Reuters\n"]}],"source":["%cd \"/content/gdrive/My Drive/reuters/hedwig-master/hedwig-data/datasets/Reuters\""]},{"cell_type":"code","source":["import pandas as pd\n","can_df = pd.read_csv(\"ca.tsv\", sep = \"\\t\", header=0)\n","ca_text = can_df[\"text\"].tolist()\n","ca_text = [str(s) for s in ca_text]\n","\n","au_df = pd.read_csv(\"au.tsv\", sep = \"\\t\", header=0)\n","au_text = au_df[\"text\"].tolist()\n","au_text = [str(s) for s in au_text]\n","\n","uk_df = pd.read_csv(\"uk.tsv\", sep = \"\\t\", header=0)\n","uk_text = uk_df[\"text\"].tolist()\n","uk_text = [str(s) for s in uk_text]\n","\n","us_df = pd.read_csv(\"us.tsv\", sep = \"\\t\", header=0)\n","us_text = us_df[\"text\"].tolist()\n","us_text = [str(s) for s in us_text]\n","\n","mtp_df = pd.read_csv(\"mtp.tsv\", sep = \"\\t\", header=0)\n","mtp_text = mtp_df[\"text\"].tolist()\n","mtp_text = [str(s) for s in mtp_text]\n","\n","test_df = pd.read_csv(\"test.tsv\", sep = \"\\t\", header=0)\n","test_text = test_df[\"text\"].tolist()\n","test_text = [str(s) for s in test_text]\n","\n","train_df = pd.read_csv(\"train.tsv\", sep = \"\\t\", header=0)\n","train_text = test_df[\"text\"].tolist()\n","train_text = [str(s) for s in train_text]"],"metadata":{"id":"902PTLOawq_8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.utils import resample\n","ca_text_clean =  resample(ca_text, n_samples = 100, random_state = 999)\n","au_text_clean =  resample(au_text, n_samples = 35, random_state = 999)\n","uk_text_clean =  resample(uk_text, n_samples = 100, random_state = 999)\n","us_text_clean =  resample(us_text, n_samples = 100, random_state = 999)\n","mtp_text_clean =  resample(mtp_text, n_samples = 100, random_state = 999)\n","test_text_clean =  resample(test_text, n_samples = 100, random_state = 999)\n","#train_text_clean =  resample(train_text, n_samples = 1000, random_state = 999)"],"metadata":{"id":"zyu7fmcOxwVt"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ShuTOFwbdpC6"},"outputs":[],"source":["from sklearn.utils import resample\n","import re\n","samp_50k_df = pd.concat([can_df, au_df, nz_df, uk_df, us_df, za_df], ignore_index=True, sort=False )\n","base_50k, base_50k_lab = resample(text_list, lab_list, random_state = 909, n_samples = 810, replace= False)\n","samp_50k_lab = [*ca_lab2,*au_lab2,*nz_lab2,*uk_lab2,*us_lab2, *za_lab2]\n","base_clean = [' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",t).split()) for t in base_50k]\n","samp_50k = [*ca_text_clean2,*au_text_clean2,*nz_text_clean2,*uk_text_clean2,*us_text_clean2, *za_text_clean2]\n","rands_text, rands_lab = resample(samp_50k, samp_50k_lab, random_state = 327, n_samples = 200, replace = False)\n","base_clean, base_lab = resample(base_clean, base_50k_lab, random_state = 327, n_samples = 200, replace = False)\n","#strat_text, strat_lab = resample(samp_50k, samp_50k_lab, random_state = 327, n_samples = 810, stratify = df_50k[\"country\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b2UfSMJbFT7S"},"outputs":[],"source":["#word_batch = tf.convert_to_tensor(text_lists, dtype = None, name = \"sentence\")\n","#word_outputs = model(word_batch)\n","def embeds_from_pretrainedBERT(text_lists):\n","  text_input = tf.convert_to_tensor(text_lists, dtype = None, name = \"sentence\")\n","  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n","  encoder_inputs = preprocessing_layer(text_input)\n","  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n","  outputs = encoder(encoder_inputs)\n","  #net = outputs['pooled_output']\n","  #net = tf.keras.layers.Dropout(0.1)(net)\n","  #net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n","  #return outputs['sequence_output']\n","  return outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IOnQiOsNNwmO"},"outputs":[],"source":["def cuts_along_length(text_lists, tensor):\n","  arr_full = tensor.numpy()\n","  arr_cuts = []\n","  len_info = []\n","  assert len(text_lists) == arr_full.shape[0]\n","  for texts, arr in zip(text_lists, arr_full):\n","    length = len(texts)\n","    cut = arr[0:length]\n","    arr_cuts.append(cut)\n","    len_info.append(length)\n","\n","  return arr_cuts, len_info"]},{"cell_type":"code","source":["def concat_tens(tensors):\n","  arrs = []\n","  for arr in tensors:\n","    arrs.append(arr)\n","  return arrs"],"metadata":{"id":"0Y0oc81Th3fm"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1697071359154,"user":{"displayName":"Annie Lyu","userId":"12460954671218305598"},"user_tz":-780},"id":"NeUXBHSCe00H","outputId":"32525328-0728-40bb-b12a-64717112de05"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/new results\n"]}],"source":["%cd '/content/gdrive/My Drive/new results'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ICy8l_GExhEp"},"outputs":[],"source":["import torch\n","def wasserstein_metric_one(arrs1, arrs2, patch = False, mscale = False):\n","    mat1 = np.concatenate(arrs1, axis = 0)\n","    mat2 = np.concatenate(arrs2, axis = 0)\n","    tens1 = torch.from_numpy(mat1)\n","    tens2 = torch.from_numpy(mat2)\n","    loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.05, backend = \"online\")\n","    if patch:\n","      loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.05, backend = \"tensorized\")\n","    if mscale:\n","      loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.05, backend = \"multiscale\")\n","    L = loss(tens1, tens2)\n","    dist_arr = L.cpu().detach().numpy()\n","    return(dist_arr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j1kqOIwd1AN4"},"outputs":[],"source":["def wasserstein_metric(tens1, tens2, patch = False, mscale = False):\n","    loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.05, backend = \"online\")\n","    if patch:\n","      loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.05, backend = \"tensorized\")\n","    if mscale:\n","      loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.05, backend = \"multiscale\")\n","    L = loss(tens1, tens2)\n","    dist_arr = L.cpu().detach().numpy()\n","    return(dist_arr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3tboBDZt9B9J"},"outputs":[],"source":["def distance_scores(data1,data2, a=3, b=5):\n","  overall = []\n","  for i in range(a):\n","    text_1 = resample(data1, n_samples = 25)\n","    text_2 = resample(data2, n_samples = 25)\n","    dists = []\n","    for j in range(b):\n","      text1 = resample(text_1, n_samples = 16)\n","      text2 = resample(text_2, n_samples = 16)\n","      embed1 = embeds_from_pretrainedBERT(text1)[\"sequence_output\"]\n","      embed2 = embeds_from_pretrainedBERT(text2)[\"sequence_output\"]\n","      cuts1, lens1 = cuts_along_length(text1,embed1)\n","      cuts2, lens2 = cuts_along_length(text2,embed2)\n","      dist = wasserstein_metric_one(cuts1, cuts2)\n","      dists.append(dist)\n","    overall.append(dists)\n","  return(overall)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bI1mDeEBzePH"},"outputs":[],"source":["def distance_scores_128(data1,data2, a=5, b=5):\n","  overall = []\n","  for i in range(10):\n","    text_1 = resample(data1, n_samples = 25)\n","    text_2 = resample(data2, n_samples = 25)\n","    dists = []\n","    for j in range(10):\n","      text1 = resample(text_1, n_samples = 16)\n","      text2 = resample(text_2, n_samples = 16)\n","      embed1 = embeds_from_pretrainedBERT(text1)[\"sequence_output\"]\n","      embed2 = embeds_from_pretrainedBERT(text2)[\"sequence_output\"]\n","      torch1 = torch.from_numpy(embed1.numpy())\n","      torch1 = concat_tens(torch1)\n","      torch2 = torch.from_numpy(embed2.numpy())\n","      torch2 = concat_tens(torch2)\n","      dist = wasserstein_metric_one(torch1, torch2)\n","      dists.append(dist)\n","    overall.append(dists)\n","  return(overall)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0b1gJ-qgRcXL"},"outputs":[],"source":["def score_average(l):\n","  flat = [i for j in l for i in j]\n","  avg = sum(flat)/len(flat)\n","  return avg, flat"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5DOO12ZUUdKc"},"outputs":[],"source":["def output_result(score1, score2):\n","  avg_score_cuts = score_average(score1)\n","  avg_score_128 = score_average(score2)\n"]},{"cell_type":"code","source":["def create_long(text):\n","  passage = []\n","  for i in range(100):\n","    text0 = resample(text, n_samples = 10, replace = False)\n","    text1 = \" \".join(text0)\n","    passage.append(text1)\n","  return passage"],"metadata":{"id":"6vtOImcDTvUx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#train vs US\n","score_128 = distance_scores_128(train_text_clean,us_text_clean)\n","#avg_score_cuts = score_average(score_cuts)\n","avg_score_128, scores = score_average(score_128)\n","scores = [s.item() for s in scores]\n","with open(\"Bert-train-US-withAU.txt\", \"w\") as outs:\n","  #outs.write(\"cuts average:\" + str(avg_score_cuts) + \" : \\n\")\n","  #for score in score_cuts:\n","    #outs.write(f\"{score}\\n\")\n","  outs.write(\"128-fix average:\" + str(avg_score_128) + \" : \\n\")\n","  outs.write(str(scores))"],"metadata":{"id":"5vyyClOJwThA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#train vs test\n","score_128 = distance_scores_128(train_text_clean,test_text_clean)\n","#avg_score_cuts = score_average(score_cuts)\n","avg_score_128, scores = score_average(score_128)\n","scores = [s.item() for s in scores]\n","with open(\"Bert-train-test-withAU.txt\", \"w\") as outs:\n","  #outs.write(\"cuts average:\" + str(avg_score_cuts) + \" : \\n\")\n","  #for score in score_cuts:\n","    #outs.write(f\"{score}\\n\")\n","  outs.write(\"128-fix average:\" + str(avg_score_128) + \" : \\n\")\n","  outs.write(str(scores))"],"metadata":{"id":"mBhjKhBtwj4Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#train vs train\n","score_128 = distance_scores_128(train_text_clean,train_text_clean)\n","#avg_score_cuts = score_average(score_cuts)\n","avg_score_128, scores = score_average(score_128)\n","scores = [s.item() for s in scores]\n","with open(\"Bert-train-train-withAU.txt\", \"w\") as outs:\n","  #outs.write(\"cuts average:\" + str(avg_score_cuts) + \" : \\n\")\n","  #for score in score_cuts:\n","    #outs.write(f\"{score}\\n\")\n","  outs.write(\"128-fix average:\" + str(avg_score_128) + \" : \\n\")\n","  outs.write(str(scores))"],"metadata":{"id":"CMxRytn9yFQK"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8X8jTZZHMPdE"},"outputs":[],"source":["#avg_score_cuts = score_average(score_cuts)\n","avg_score_128, scores = score_average(score_128)"]},{"cell_type":"code","source":["scores = [s.item() for s in scores]"],"metadata":{"id":"_ZYzDXFqDjft"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_aUGGaqRSLV6"},"outputs":[],"source":["with open(\"Bert-test-test-reuters-withAU-new.txt\", \"w\") as outs:\n","  #outs.write(\"cuts average:\" + str(avg_score_cuts) + \" : \\n\")\n","  #for score in score_cuts:\n","    #outs.write(f\"{score}\\n\")\n","  outs.write(\"128-fix average:\" + str(avg_score_128) + \" : \\n\")\n","  outs.write(str(scores))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"STOHrxVIVvTI"},"outputs":[],"source":["#rand vs CA:\n","#score_cuts = distance_scores(samp_50k,ca_text_clean)\n","score_128 = distance_scores_128(test_text_clean,ca_text_clean)\n","#avg_score_cuts = score_average(score_cuts)\n","avg_score_128, scores = score_average(score_128)\n","scores = [s.item() for s in scores]\n","with open(\"Bert-test-CA2-reuters-withAU-new.txt\", \"w\") as outs:\n","  #outs.write(\"cuts average:\" + str(avg_score_cuts) + \" : \\n\")\n","  #for score in score_cuts:\n","    #outs.write(f\"{score}\\n\")\n","  outs.write(\"128-fix average:\" + str(avg_score_128) + \" : \\n\")\n","  outs.write(str(scores))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ERh0xPQQReJ"},"outputs":[],"source":["#rand vs AU:\n","#score_cuts = distance_scores(samp_50k,au_text_clean)\n","score_128 = distance_scores_128(test_text_clean,au_text_clean)\n","#avg_score_cuts = score_average(score_cuts)\n","avg_score_128, scores = score_average(score_128)\n","scores = [s.item() for s in scores]\n","with open(\"Bert-test-AU-reuters-withAU-new.txt\", \"w\") as outs:\n","  #outs.write(\"cuts average:\" + str(avg_score_cuts) + \" : \\n\")\n","  #for score in score_cuts:\n","  #  outs.write(f\"{score}\\n\")\n","  outs.write(\"128-fix average:\" + str(avg_score_128) + \" : \\n\")\n","  outs.write(str(scores))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ajAncWF4WFPu"},"outputs":[],"source":["#rand vs UK:\n","#core_cuts = distance_scores(samp_50k,uk_text_clean)\n","score_128 = distance_scores_128(test_text_clean,uk_text_clean)\n","#avg_score_cuts = score_average(score_cuts)\n","avg_score_128, scores = score_average(score_128)\n","scores = [s.item() for s in scores]\n","\n","with open(\"Bert-test-UK-reuters-withAU-new.txt\", \"w\") as outs:\n","  #outs.write(\"cuts average:\" + str(avg_score_cuts) + \" : \\n\")\n","  #for score in score_cuts:\n","  #  outs.write(f\"{score}\\n\")\n","  outs.write(\"128-fix average:\" + str(avg_score_128) + \" : \\n\")\n","  outs.write(str(scores))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PTVOUDq0WLJe"},"outputs":[],"source":["#rand vs US:\n","#score_cuts = distance_scores(samp_50k,us_text_clean)\n","score_128 = distance_scores_128(test_text_clean,us_text_clean)\n","#avg_score_cuts = score_average(score_cuts)\n","avg_score_128, scores = score_average(score_128)\n","scores = [s.item() for s in scores]\n","\n","with open(\"Bert-test-US-reuters-withAU-new.txt\", \"w\") as outs:\n","  #outs.write(\"cuts average:\" + str(avg_score_cuts) + \" : \\n\")\n","  #for score in score_cuts:\n","  #  outs.write(f\"{score}\\n\")\n","  outs.write(\"128-fix average:\" + str(avg_score_128) + \" : \\n\")\n","  outs.write(str(scores))"]},{"cell_type":"code","source":["score_128 = distance_scores_128(test_text_clean,mtp_text_clean)\n","#avg_score_cuts = score_average(score_cuts)\n","avg_score_128, scores = score_average(score_128)\n","scores = [s.item() for s in scores]\n","\n","with open(\"Bert-test-MTP-reuters-withAU-new.txt\", \"w\") as outs:\n","  #outs.write(\"cuts average:\" + str(avg_score_cuts) + \" : \\n\")\n","  #for score in score_cuts:\n","  #  outs.write(f\"{score}\\n\")\n","  outs.write(\"128-fix average:\" + str(avg_score_128) + \" : \\n\")\n","  outs.write(str(scores))"],"metadata":{"id":"7pdEkjek8w2O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#5mixed vs CA\n","#score_cuts = distance_scores(samp_50k,ca_text_clean)\n","score_128 = distance_scores_128(mtp_text_clean,ca_text_clean)\n","#avg_score_cuts = score_average(score_cuts)\n","avg_score_128, scores = score_average(score_128)\n","scores = [s.item() for s in scores]\n","with open(\"Bert-MTP2-CA-reuters-withAU.txt\", \"w\") as outs:\n","  #outs.write(\"cuts average:\" + str(avg_score_cuts) + \" : \\n\")\n","  #for score in score_cuts:\n","    #outs.write(f\"{score}\\n\")\n","  outs.write(\"128-fix average:\" + str(avg_score_128) + \" : \\n\")\n","  outs.write(str(scores))"],"metadata":{"id":"YYymzpy5fpKG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#5mixed vs AU\n","#score_cuts = distance_scores(samp_50k,ca_text_clean)\n","score_128 = distance_scores_128(mtp_text_clean,au_text_clean)\n","#avg_score_cuts = score_average(score_cuts)\n","avg_score_128, scores = score_average(score_128)\n","scores = [s.item() for s in scores]\n","with open(\"Bert-MTP2-AU-reuters-withAU.txt\", \"w\") as outs:\n","  #outs.write(\"cuts average:\" + str(avg_score_cuts) + \" : \\n\")\n","  #for score in score_cuts:\n","    #outs.write(f\"{score}\\n\")\n","  outs.write(\"128-fix average:\" + str(avg_score_128) + \" : \\n\")\n","  outs.write(str(scores))"],"metadata":{"id":"Rte4HBmkfsFC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#5mixed vs UK\n","#score_cuts = distance_scores(samp_50k,ca_text_clean)\n","score_128 = distance_scores_128(mtp_text_clean,uk_text_clean)\n","#avg_score_cuts = score_average(score_cuts)\n","avg_score_128, scores = score_average(score_128)\n","scores = [s.item() for s in scores]\n","with open(\"Bert-MTP2-UK-withAU-new.txt\", \"w\") as outs:\n","  #outs.write(\"cuts average:\" + str(avg_score_cuts) + \" : \\n\")\n","  #for score in score_cuts:\n","    #outs.write(f\"{score}\\n\")\n","  outs.write(\"128-fix average:\" + str(avg_score_128) + \" : \\n\")\n","  outs.write(str(scores))"],"metadata":{"id":"4HAw2QVUfxKy","executionInfo":{"status":"ok","timestamp":1700628557243,"user_tz":-780,"elapsed":1428368,"user":{"displayName":"Annie Lyu","userId":"12460954671218305598"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"aa7debd1-e4b3-44cb-fb46-b9a13cdc4411"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[KeOps] Generating code for formula Max_SumShiftExpWeight_Reduction(Concat(Var(2,1,1)-Var(3,1,2)*(((Var(0,512,0)-Var(1,512,1))|(Var(0,512,0)-Var(1,512,1)))/2),1),0) ... OK\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"BABsmhlYF4ZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#5mixed vs US\n","score_128 = distance_scores_128(mtp_text_clean,us_text_clean)\n","#avg_score_cuts = score_average(score_cuts)\n","avg_score_128, scores = score_average(score_128)\n","scores = [s.item() for s in scores]\n","with open(\"Bert-MTP2-US-withAU-new.txt\", \"w\") as outs:\n","  #outs.write(\"cuts average:\" + str(avg_score_cuts) + \" : \\n\")\n","  #for score in score_cuts:\n","    #outs.write(f\"{score}\\n\")\n","  outs.write(\"128-fix average:\" + str(avg_score_128) + \" : \\n\")\n","  outs.write(str(scores))"],"metadata":{"id":"tBsn0aB4f0Xa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#5mixed vs 5mixed\n","score_128 = distance_scores_128(mtp_text_clean,mtp_text_clean)\n","#avg_score_cuts = score_average(score_cuts)\n","avg_score_128, scores = score_average(score_128)\n","scores = [s.item() for s in scores]\n","with open(\"Bert-MTP-MTP-withAU.txt\", \"w\") as outs:\n","  #outs.write(\"cuts average:\" + str(avg_score_cuts) + \" : \\n\")\n","  #for score in score_cuts:\n","    #outs.write(f\"{score}\\n\")\n","  outs.write(\"128-fix average:\" + str(avg_score_128) + \" : \\n\")\n","  outs.write(str(scores))"],"metadata":{"id":"xr6nfFOlf2PV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#CA vs CA\n","score_128 = distance_scores_128(ca_text_clean,ca_text_clean)\n","#avg_score_cuts = score_average(score_cuts)\n","avg_score_128, scores = score_average(score_128)\n","scores = [s.item() for s in scores]\n","\n","with open(\"Bert-CA-CA-wiwthAU.txt\", \"w\") as outs:\n","  #outs.write(\"cuts average:\" + str(avg_score_cuts) + \" : \\n\")\n","  #for score in score_cuts:\n","  #  outs.write(f\"{score}\\n\")\n","  outs.write(\"128-fix average:\" + str(avg_score_128) + \" : \\n\")\n","  outs.write(str(scores))"],"metadata":{"id":"oSD16ygAf4rO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#AU vs AU\n","score_128 = distance_scores_128(au_text_clean,au_text_clean)\n","#avg_score_cuts = score_average(score_cuts)\n","avg_score_128, scores = score_average(score_128)\n","scores = [s.item() for s in scores]\n","\n","with open(\"Bert-AU-AU-withAU.txt\", \"w\") as outs:\n","  #outs.write(\"cuts average:\" + str(avg_score_cuts) + \" : \\n\")\n","  #for score in score_cuts:\n","  #  outs.write(f\"{score}\\n\")\n","  outs.write(\"128-fix average:\" + str(avg_score_128) + \" : \\n\")\n","  outs.write(str(scores))"],"metadata":{"id":"KFjzu6ATf6dL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KYCOcTX0RZFN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#UK vs UK\n","score_128 = distance_scores_128(uk_text_clean,uk_text_clean)\n","#avg_score_cuts = score_average(score_cuts)\n","avg_score_128, scores = score_average(score_128)\n","scores = [s.item() for s in scores]\n","\n","with open(\"Bert-UK-UK-withAU.txt\", \"w\") as outs:\n","  #outs.write(\"cuts average:\" + str(avg_score_cuts) + \" : \\n\")\n","  #for score in score_cuts:\n","  #  outs.write(f\"{score}\\n\")\n","  outs.write(\"128-fix average:\" + str(avg_score_128) + \" : \\n\")\n","  outs.write(str(scores))"],"metadata":{"id":"jBdlseLPf9cq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#US vs US\n","score_128 = distance_scores_128(us_text_clean,us_text_clean)\n","#avg_score_cuts = score_average(score_cuts)\n","avg_score_128, scores = score_average(score_128)\n","scores = [s.item() for s in scores]\n","\n","with open(\"Bert-US-US-withAU.txt\", \"w\") as outs:\n","  #outs.write(\"cuts average:\" + str(avg_score_cuts) + \" : \\n\")\n","  #for score in score_cuts:\n","  #  outs.write(f\"{score}\\n\")\n","  outs.write(\"128-fix average:\" + str(avg_score_128) + \" : \\n\")\n","  outs.write(str(scores))"],"metadata":{"id":"uTgH9l4AgAUg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["samp_50k, samp_50k_lab = resample(text_list, lab_list, random_state = 909, n_samples = 810, replace= False)"],"metadata":{"id":"Y4T3l1pwzqcO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#rand vs 5mixed\n","score_128 = distance_scores_128(rands_text,samp_50k)\n","#avg_score_cuts = score_average(score_cuts)\n","avg_score_128, scores = score_average(score_128)\n","scores = [s.item() for s in scores]\n","with open(\"Bert-5mixed-rand-128.txt\", \"w\") as outs:\n","  #outs.write(\"cuts average:\" + str(avg_score_cuts) + \" : \\n\")\n","  #for score in score_cuts:\n","    #outs.write(f\"{score}\\n\")\n","  outs.write(\"128-fix average:\" + str(avg_score_128) + \" : \\n\")\n","  outs.write(str(scores))"],"metadata":{"id":"hgfmHNE1zgOC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wParUKKEiMnv"},"outputs":[],"source":["#CA vs AU\n","#score_cuts = distance_scores(au_text_clean, ca_text_clean)\n","score_128 = distance_scores_128(au_text_clean,ca_text_clean)\n","#avg_score_cuts = score_average(score_cuts)\n","avg_score_128, scores = score_average(score_128)\n","scores = [s.item() for s in scores]\n","\n","with open(\"Bert-CA-AU-withAU.txt\", \"w\") as outs:\n","  #outs.write(\"cuts average:\" + str(avg_score_cuts) + \" : \\n\")\n","  #for score in score_cuts:\n","  #  outs.write(f\"{score}\\n\")\n","  outs.write(\"128-fix average:\" + str(avg_score_128) + \" : \\n\")\n","  outs.write(str(scores))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fBsb-klyiStG"},"outputs":[],"source":["#CA vs UK\n","#score_cuts = distance_scores(uk_text_clean, ca_text_clean)\n","score_128 = distance_scores_128(uk_text_clean,ca_text_clean)\n","#avg_score_cuts = score_average(score_cuts)\n","avg_score_128, scores = score_average(score_128)\n","scores = [s.item() for s in scores]\n","with open(\"Bert-CA-UK-withAU.txt\", \"w\") as outs:\n","  #outs.write(\"cuts average:\" + str(avg_score_cuts) + \" : \\n\")\n","  #for score in score_cuts:\n","  #  outs.write(f\"{score}\\n\")\n","  outs.write(\"128-fix average:\" + str(avg_score_128) + \" : \\n\")\n","  outs.write(str(scores))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fxz9jH9TiWIG"},"outputs":[],"source":["#CA vs US\n","#score_cuts = distance_scores(us_text_clean, ca_text_clean)\n","score_128 = distance_scores_128(us_text_clean,ca_text_clean)\n","#avg_score_cuts = score_average(score_cuts)\n","avg_score_128, scores = score_average(score_128)\n","scores = [s.item() for s in scores]\n","\n","with open(\"Bert-CA-US-withAU.txt\", \"w\") as outs:\n","  #outs.write(\"cuts average:\" + str(avg_score_cuts) + \" : \\n\")\n","  #for score in score_cuts:\n","  #  outs.write(f\"{score}\\n\")\n","  outs.write(\"128-fix average:\" + str(avg_score_128) + \" : \\n\")\n","  outs.write(str(scores))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p9Oyotb6ia5n"},"outputs":[],"source":["#AU vs UK\n","#score_cuts = distance_scores(au_text_clean, uk_text_clean)\n","score_128 = distance_scores_128(au_text_clean, uk_text_clean)\n","#avg_score_cuts = score_average(score_cuts)\n","avg_score_128, scores = score_average(score_128)\n","scores = [s.item() for s in scores]\n","\n","with open(\"Bert-AU-UK-withAU.txt\", \"w\") as outs:\n","  #outs.write(\"cuts average:\" + str(avg_score_cuts) + \" : \\n\")\n","  #for score in score_cuts:\n","  #  outs.write(f\"{score}\\n\")\n","  outs.write(\"128-fix average:\" + str(avg_score_128) + \" : \\n\")\n","  outs.write(str(scores))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E4VybJ29ie8f"},"outputs":[],"source":["#AU vs US\n","#score_cuts = distance_scores(au_text_clean, us_text_clean)\n","score_128 = distance_scores_128(au_text_clean, us_text_clean)\n","#avg_score_cuts = score_average(score_cuts)\n","avg_score_128, scores = score_average(score_128)\n","scores = [s.item() for s in scores]\n","\n","with open(\"Bert-AU-US-withAU.txt\", \"w\") as outs:\n","  #outs.write(\"cuts average:\" + str(avg_score_cuts) + \" : \\n\")\n","  #for score in score_cuts:\n","  #  outs.write(f\"{score}\\n\")\n","  outs.write(\"128-fix average:\" + str(avg_score_128) + \" : \\n\")\n","  outs.write(str(scores))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jjyjLpe9j63v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700367822071,"user_tz":-780,"elapsed":1353115,"user":{"displayName":"Annie Lyu","userId":"12460954671218305598"}},"outputId":"2e3f9e87-b570-4bf5-e3f0-c5aa272ef966"},"outputs":[{"output_type":"stream","name":"stdout","text":["[KeOps] Generating code for formula Max_SumShiftExpWeight_Reduction(Concat(Var(2,1,1)-Var(3,1,2)*(((Var(0,512,0)-Var(1,512,1))|(Var(0,512,0)-Var(1,512,1)))/2),1),0) ... OK\n"]}],"source":["#UK vs US\n","#score_cuts = distance_scores(uk_text_clean, us_text_clean)\n","score_128 = distance_scores_128(uk_text_clean, us_text_clean)\n","#avg_score_cuts = score_average(score_cuts)\n","avg_score_128, scores = score_average(score_128)\n","scores = [s.item() for s in scores]\n","\n","with open(\"Bert-UK-US-2withAU-new.txt\", \"w\") as outs:\n","  #outs.write(\"cuts average:\" + str(avg_score_cuts) + \" : \\n\")\n","  #for score in score_cuts:\n","  #  outs.write(f\"{score}\\n\")\n","  outs.write(\"128-fix average:\" + str(avg_score_128) + \" : \\n\")\n","  outs.write(str(scores))\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"115AJ9kDA3O-xSAqgInx5hLEIKyI4H6lN","timestamp":1701738669154},{"file_id":"10lo5nA6ILFipGeZ-yB1HvQOZsNOEKxb8","timestamp":1698220334993},{"file_id":"13bWNedrRDPIGvnatPUYqsNoqPTF8MwbW","timestamp":1698201717036},{"file_id":"14wlfuiVvPb28TLS3dydbuPI44EQvRy-q","timestamp":1695353699808},{"file_id":"1mfGp63KVxYRjfvOehwHhmJZXV1jmIZR9","timestamp":1692834546156},{"file_id":"19sEln5FYubB1ME5Llh0b0xRAXf0-hTvW","timestamp":1692671799276},{"file_id":"1Ij6yFlAgtutXzfu-Dr9j_x3dORQHQ7nw","timestamp":1690328681023},{"file_id":"1jR083jod3rCzJAOMsTRxWMbUFq6zugtF","timestamp":1684292369037},{"file_id":"1xaFQnPUdAfeW1cUTlp5ZLMf5781lIYNl","timestamp":1684196025903},{"file_id":"1X5COfqjcIoYmisr2rFeOxPU_CKHCVJuk","timestamp":1684126509478},{"file_id":"1eDbzzG0lhAY998jC64tog548zLpBur_9","timestamp":1684115436958}],"machine_shape":"hm","gpuType":"V100","authorship_tag":"ABX9TyPQgQXWZQZXRZ9DI7vxdR4p"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}